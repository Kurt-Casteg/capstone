Informe Final de Proyecto de Título: Clasificación de Tejidos Histológicos de Cáncer Colorrectal mediante Aprendizaje Profundo
RESUMEN
Este informe presenta un análisis comparativo exhaustivo de arquitecturas de aprendizaje profundo para la clasificación de imágenes histológicas de cáncer colorrectal, una tarea fundamental en el campo emergente de la patología digital. La automatización de este proceso mediante inteligencia artificial no solo promete acelerar el diagnóstico, sino también mejorar su precisión y consistencia, representando una oportunidad estratégica para optimizar los flujos de trabajo clínicos y avanzar hacia una medicina de precisión.
El proyecto abordó la clasificación de ocho tipos de tejido de cáncer colorrectal mediante una evaluación comparativa sistemática de cinco arquitecturas de aprendizaje profundo: una Red Neuronal Convolucional (CNN) de línea base entrenada desde cero y cuatro modelos avanzados (VGG19, ResNet50, EfficientNetB0 y Vision Transformer) implementados con la estrategia de Transfer Learning. Los resultados demostraron que la arquitectura ResNet50 alcanzó el rendimiento más alto, con una precisión en el conjunto de prueba del 95.07%. El análisis validó la hipótesis central del proyecto: las arquitecturas pre-entrenadas superaron significativamente al modelo entrenado desde cero, subrayando la eficacia del Transfer Learning en dominios con datos limitados. Además, el estudio reveló un complejo balance entre rendimiento y eficiencia computacional, donde modelos teóricamente más eficientes como EfficientNetB0 no siempre se traducen en la menor latencia de inferencia en hardware específico. Este informe ofrece un análisis detallado de estos hallazgos, proporcionando una guía empírica para la selección de arquitecturas en aplicaciones de patología computacional.
-------------------------------------------------------------------------------- 
1. INTRODUCCIÓN
La histopatología digital está transformando el diagnóstico del cáncer, convirtiendo portaobjetos de vidrio en imágenes de gigapíxeles que pueden ser analizadas computacionalmente. Este paradigma genera un volumen de datos sin precedentes, haciendo insostenible el análisis puramente manual y creando una necesidad crítica de herramientas automatizadas que asistan a los patólogos. Este proyecto se enmarca en esa necesidad estratégica, buscando desarrollar y validar un sistema de clasificación de tejidos robusto y eficiente.
El problema central de este estudio es el análisis de imágenes histológicas de cáncer colorrectal. El trabajo base de Kather, et al. [1] estableció que el análisis de texturas es fundamental para diferenciar tipos de tejido, como tumor, estroma o linfocitos. Sin embargo, los métodos clásicos de análisis de imágenes, que dependen de características diseñadas manualmente, muestran limitaciones en su poder de generalización. Un desafío adicional es la alta variabilidad en la tinción de Hematoxilina y Eosina (H&E) entre diferentes laboratorios, un factor que, como señala Tellez, et al. [2], puede degradar significativamente el rendimiento de los modelos si no se aborda adecuadamente.
En la última década, las Redes Neuronales Convolucionales (CNNs) han surgido como la solución de vanguardia para problemas de visión computacional, incluyendo el análisis de imágenes médicas [3]. A diferencia de los métodos tradicionales, las CNNs tienen la capacidad de aprender jerarquías complejas de características espaciales directamente de los datos, eliminando la necesidad de una ingeniería de características manual y propensa a errores. Esta capacidad de aprendizaje automático las hace inherentemente más robustas a las variaciones presentes en los datos del mundo real.
A pesar del reconocido potencial de las CNNs, no existe un consenso claro sobre qué arquitectura específica ofrece el mejor balance entre precisión de clasificación y eficiencia computacional para esta tarea. Arquitecturas con conexiones residuales (ResNet), con escalado compuesto (EfficientNet) o basadas en mecanismos de atención (Vision Transformers) proponen soluciones fundamentalmente distintas al problema de la representación visual. Por lo tanto, este proyecto se justifica en la necesidad de realizar una evaluación rigurosa y comparativa de estas arquitecturas de vanguardia para llenar este vacío de conocimiento. El objetivo es determinar no solo el modelo más preciso, sino también el más eficiente, un factor clave para su eventual despliegue en entornos clínicos.
Este informe está estructurado para guiar al lector desde los fundamentos teóricos hasta los hallazgos experimentales. La sección 2 revisa el estado del arte y el marco teórico que sustenta el proyecto. La sección 3 presenta la hipótesis y los objetivos específicos. La sección 4 detalla la metodología, incluyendo el conjunto de datos y la implementación de cada modelo. La sección 5 presenta y analiza los resultados comparativos. Finalmente, la sección 6 extrae las conclusiones, valida la hipótesis y propone líneas de investigación futuras. Este recorrido proporciona una base sólida para comprender las decisiones metodológicas y la relevancia de los resultados obtenidos.
2. TRABAJO RELACIONADO Y MARCO TEÓRICO
La revisión del estado del arte es un pilar fundamental de este proyecto. Esta sección no solo contextualiza el trabajo dentro del campo de la visión computacional aplicada a la histopatología, sino que también fundamenta cada una de las decisiones metodológicas adoptadas. Desde la selección de arquitecturas de aprendizaje profundo hasta las estrategias de entrenamiento y regularización, los avances más significativos del campo han guiado el diseño experimental para asegurar su relevancia y rigor científico.
2.1. Evolución del Análisis de Imágenes Histológicas
1.	Métodos Clásicos de Análisis de Textura: Históricamente, el análisis de imágenes histológicas se basaba en la extracción de características de textura diseñadas manualmente. El trabajo de Kather, et al. [1], que sirve como base para este proyecto, evaluó exhaustivamente estos enfoques. Entre los descriptores más comunes se encuentran los patrones binarios locales (LBP), que codifican la estructura local de la textura; la matriz de co-ocurrencia de niveles de gris (GLCM), que captura relaciones espaciales entre píxeles; y los filtros de Gabor, que analizan la frecuencia y orientación de la textura. Si bien estos métodos demostraron ser útiles y sentaron las bases del campo, su dependencia de características predefinidas limita su capacidad para capturar la complejidad morfológica del tejido. En el estudio de Kather [1], la combinación de estos descriptores alcanzó una precisión máxima del 87.4%, un rendimiento notable pero superado por los enfoques modernos.
2.	El Paradigma de las Redes Neuronales Convolucionales (CNNs): La transición hacia el aprendizaje profundo marcó un cambio de paradigma. Las CNNs, como se describe en la revisión de Yamashita, et al. [3], automatizan el proceso de extracción de características. En lugar de definir manualmente los descriptores, una CNN aprende una jerarquía de filtros (o kernels) que se optimizan para la tarea específica. Las primeras capas aprenden características simples como bordes y colores, mientras que las capas más profundas combinan estas características para reconocer patrones complejos, como núcleos celulares o estructuras glandulares. Un hito en esta evolución fue la arquitectura VGG (Simonyan & Zisserman [4]), que demostró que el uso de filtros convolucionales muy pequeños (3x3) apilados en redes profundas podía lograr un rendimiento sobresaliente, estableciendo un principio de diseño que influyó en muchas arquitecturas posteriores.
2.2. Arquitecturas Avanzadas de Aprendizaje Profundo
1.	VGGNets: Propuestas por Simonyan & Zisserman [3], las redes VGG demostraron que la profundidad es un componente crítico para el rendimiento. Su principal contribución fue el uso exclusivo de filtros convolucionales muy pequeños (3x3) apilados en múltiples capas, lo que permitió construir arquitecturas significativamente más profundas (16-19 capas) que sus predecesoras, logrando una mejora sustancial en la capacidad de representación del modelo
2.	Redes Residuales (ResNet): Uno de los mayores desafíos al entrenar redes muy profundas es el problema de la desaparición del gradiente (vanishing gradient), donde la señal de error se debilita a medida que se propaga hacia atrás. La arquitectura ResNet, propuesta por He, et al. [5], introdujo una solución elegante: las conexiones residuales o skip connections. Estas conexiones crean un "atajo" que permite que el gradiente fluya directamente a través de las capas, facilitando el entrenamiento de redes con cientos o incluso miles de capas. Esta capacidad es crucial para el análisis de imágenes médicas, donde la profundidad permite al modelo aprender las sutiles características jerárquicas necesarias para una clasificación precisa.
3.	Redes Eficientes (EfficientNet): Tradicionalmente, para mejorar una CNN, los investigadores escalaban una de sus tres dimensiones: profundidad (más capas), ancho (más filtros) o resolución de la imagen de entrada. EfficientNet, de Tan & Le [6], propuso un enfoque más sistemático llamado escalado compuesto. Esta técnica utiliza un coeficiente para escalar de manera balanceada y simultánea las tres dimensiones. El resultado es una familia de modelos, como EfficientNetB0, que logran una precisión muy alta con una eficiencia computacional sin precedentes en términos de parámetros y operaciones de punto flotante (FLOPs). Este concepto es central para la hipótesis de este proyecto, que postula un balance óptimo entre rendimiento y eficiencia.
4.	Vision Transformers (ViT): Inspirada en el éxito de la arquitectura Transformer en el procesamiento del lenguaje natural [7], la arquitectura Vision Transformer (ViT) fue adaptada a la visión por computadora por Dosovitskiy, et al. [8]. A diferencia de las CNNs, que procesan la información localmente a través de convoluciones, los ViT dividen la imagen en una secuencia de parches (patches). Luego, aplican un mecanismo de auto-atención (self-attention) que permite a cada parche ponderar la importancia de todos los demás parches de la imagen. Esto le confiere un campo receptivo global desde la primera capa, permitiéndole modelar dependencias a larga distancia de manera más efectiva que las CNNs.
2.3. Estrategias de Entrenamiento para Imágenes Médicas
1.	Aprendizaje por Transferencia (Transfer Learning): El Aprendizaje por Transferencia es una técnica fundamental, especialmente en el dominio médico donde los conjuntos de datos suelen ser limitados. Como explican Yamashita, et al. [3], esta estrategia consiste en tomar un modelo pre-entrenado en un dataset a gran escala, como ImageNet, y adaptarlo a una nueva tarea. La intuición es que las características de bajo nivel aprendidas de imágenes naturales (bordes, texturas, formas) son universales y transferibles al dominio médico. Utilizar estos pesos pre-entrenados como punto de partida, en lugar de inicializar el modelo desde cero, acelera la convergencia y a menudo conduce a un mejor rendimiento final.
2.	Regularización y Optimización: Para evitar el sobreajuste (overfitting), se utilizan técnicas de regularización como Dropout (desactivación aleatoria de neuronas durante el entrenamiento) y decaimiento de peso (L2), que penaliza los pesos grandes en el modelo. La optimización es gestionada por algoritmos como Adam y su variante AdamW para Vision Transformer, que adaptan la tasa de aprendizaje para cada parámetro. Además, se emplean esquemas de tasa de aprendizaje sofisticados. Uno de los más efectivos, especialmente para modelos complejos como los Transformers, combina un calentamiento inicial (warmup) con un decaimiento cosenoidal. La fase de warmup, popularizada por Goyal, et al. [10], comienza con una tasa de aprendizaje muy baja para estabilizar el entrenamiento en sus primeras etapas, especialmente con lotes grandes. Posteriormente, la tasa se reduce gradualmente siguiendo una curva cosenoidal, técnica conocida como cosine annealing y analizada por Loshchilov & Hutter [9], lo que ayuda al modelo a converger a mínimos más precisos en la función de pérdida.
3.	Aumento y Normalización de Datos Específicos para Histopatología: El aumento de datos es crucial para mejorar la generalización. Para imágenes histológicas, las transformaciones geométricas como rotaciones y volteos son particularmente importantes, ya que la orientación de los tejidos en un portaobjetos es arbitraria. Además, la variabilidad del color debido a las diferencias en los protocolos de tinción H&E es un desafío significativo. Tellez, et al. [2] demostraron que el aumento de color (por ejemplo, en el espacio de color HSV) y la normalización de color (algoritmos que estandarizan la apariencia de la tinción) son estrategias críticas. Su trabajo concluye que la combinación de ambas técnicas suele ofrecer el mejor rendimiento, al hacer que el modelo sea robusto a una amplia gama de variaciones de tinción.
Esta base teórica, que abarca desde arquitecturas de vanguardia hasta estrategias de entrenamiento específicas del dominio, proporciona el andamiaje necesario para formular y poner a prueba la hipótesis y los objetivos de este proyecto.
3. HIPÓTESIS Y OBJETIVOS
Con base en el marco teórico y el estado del arte, se establecen la hipótesis central y los objetivos que guían la fase experimental de este proyecto. Estos elementos definen formalmente el alcance de la investigación, los métodos a emplear y los criterios que determinarán el éxito del estudio.
3.1. Hipótesis
"La aplicación de técnicas de Transfer Learning pre-entrenadas en ImageNet permitirá alcanzar niveles de precisión superiores y convergencia más estable en comparación con entrenamiento desde cero (from scratch), dado el tamaño limitado de datasets histológicos. Específicamente, las arquitecturas CNN optimizadas (EfficientNet) ofrecerán el mejor balance entre rendimiento de clasificación y eficiencia computacional (menor tiempo de inferencia y FLOPs), superando a arquitecturas pesadas (VGG) o Vision Transformers, que requerirán costo computacional desproporcionado respecto a la ganancia marginal en precisión."
3.2. Objetivo General
"Evaluar y comparar el desempeño de distintas arquitecturas de aprendizaje profundo (CNNs y Vision Transformers) y estrategias de entrenamiento (Transfer Learning vs. from scratch), para determinar cuál ofrece el mejor balance entre precisión de clasificación y eficiencia computacional en análisis de tejidos histológicos de cáncer colorrectal."
3.3. Objetivos Específicos
Para alcanzar el objetivo general, se definen los siguientes objetivos específicos:
1.	Implementar cuatro arquitecturas (ResNet, VGG, EfficientNet, Vision Transformers) aplicando Transfer Learning y una arquitectura de base con entrenamiento from scratch para la clasificación de imágenes histológicas.
2.	Evaluar el rendimiento de clasificación y la capacidad de generalización de cada modelo mediante métricas clave (Accuracy, F1-Score) y el análisis de curvas de aprendizaje para identificar sobreajuste.
3.	Medir la eficiencia computacional de cada arquitectura en términos de FLOPs, tiempo de inferencia y consumo de memoria.
4.	Determinar la significancia estadística de las diferencias de rendimiento entre los modelos, identificando la arquitectura que maximiza la relación costo-beneficio para esta aplicación.
La consecución de estos objetivos permitirá validar o refutar la hipótesis planteada, proporcionando una guía empírica para la selección de modelos en futuras aplicaciones de patología computacional.
4. DATOS Y METODOLOGÍA
Esta sección detalla con rigor técnico el conjunto de datos utilizado, los pasos de preprocesamiento aplicados y la configuración específica de cada uno de los modelos evaluados. El propósito es describir de manera explícita el diseño experimental, asegurando la transparencia y la reproducibilidad de este estudio.
4.1. Conjunto de Datos y Preprocesamiento
1.	Origen y Características: El estudio se basa en el conjunto de datos público "Kather_texture_2016_image_tiles_5000", publicado por Kather, et al. [1]. Este dataset contiene un total de 5,000 imágenes de tejido histológico de cáncer colorrectal, extraídas de portaobjetos digitales teñidos con H&E. Cada imagen es un parche (o tile) con dimensiones de 150x150 píxeles en formato TIFF.
2.	Estructura de Clases: El conjunto de datos está organizado en 8 clases de tejido, cada una representando un tipo morfológico distinto:
o	01_TUMOR: Tejido tumoral/canceroso
o	02_STROMA: Tejido conectivo que rodea al tumor
o	03_COMPLEX: Estructuras de tejido mixto o complejas
o	04_LYMPHO: Agregados de linfocitos (células inmunes)
o	05_DEBRIS: Desechos celulares y material necrótico
o	06_MUCOSA: Tejido mucoso normal
o	07_ADIPOSE: Tejido adiposo (grasa)
o	08_EMPTY: Regiones vacías o fondo del portaobjetos
3.	Análisis Exploratorio de Datos (EDA): Antes del entrenamiento, se realizó un análisis exhaustivo de los datos, con los siguientes hallazgos clave:
o	Balance de Clases: El dataset está perfectamente balanceado, con exactamente 625 imágenes por cada una de las 8 clases. Esto elimina la necesidad de técnicas de re-muestreo para corregir desbalances.
o	Integridad de Datos: Se realizó una validación de todos los archivos, la cual arrojó una tasa de aprobación del 100%, confirmando que no existen imágenes corruptas o con dimensiones incorrectas.
o	Análisis de Separabilidad: Se aplicó un Análisis de Componentes Principales (PCA) sobre los valores de píxeles crudos para evaluar la separabilidad lineal de las clases. Los dos primeros componentes principales solo lograron explicar el 72.79% de la varianza total, y la proyección visual mostró una superposición considerable entre las clases. Este resultado justifica la necesidad de utilizar modelos no lineales y de aprendizaje profundo, capaces de aprender jerarquías de características complejas que no son linealmente separables.
4.	División de Datos: El conjunto de datos se dividió de manera estratificada para asegurar que la distribución de clases se mantuviera constante en todas las particiones. Se utilizó una proporción de 70/15/15, resultando en la siguiente distribución:
o	Entrenamiento: 3,500 imágenes
o	Validación: 750 imágenes
o	Prueba: 750 imágenes
5.	Esta división garantiza que el modelo se entrena con una representación diversa de los datos, se ajusta con un conjunto de validación independiente y su rendimiento final se evalúa sobre datos completamente nuevos para el modelo.
4.2. Arquitecturas y Estrategias de Entrenamiento
A continuación, se describe la implementación de cada uno de los cinco modelos evaluados.
4.2.1. Modelo Base: CNN Mejorada (Entrenamiento desde Cero)
•	Justificación: Implementé este modelo para establecer una línea base de rendimiento. Su propósito es demostrar el desempeño de una arquitectura convolucional estándar, entrenada exclusivamente con los datos del proyecto, contra la cual se compararán los modelos de Transfer Learning.
•	Arquitectura: Diseñé una CNN personalizada compuesta por 4 bloques convolucionales. El número de filtros aumenta progresivamente (32 → 64 → 128 → 256) para capturar características cada vez más complejas. Cada bloque contiene dos capas Conv2D de 3x3 con regularización L2, una capa de BatchNormalization para estabilizar el entrenamiento, una activación ReLU y una capa de MaxPooling de 2x2 para reducir la dimensionalidad espacial. Para la regularización, incluí SpatialDropout (0.15) después de cada bloque y Dropout (0.25). El cabezal de clasificación se compone de una capa GlobalAveragePooling2D, una capa densa de 256 unidades con fuerte regularización L2 (3e-4) y un Dropout final de 0.6.
•	Pipeline de Datos: Este modelo utiliza las imágenes en su tamaño nativo de 150x150 píxeles. La estrategia de aumento de datos fue diseñada específicamente para histopatología: apliqué aumento de color en el espacio HSV (hue_delta=0.15, saturation_range=[0.6, 1.4]) para simular la variabilidad de la tinción y aumento geométrico (rotaciones de 90 grados y volteos horizontales/verticales) para lograr invarianza a la orientación.
•	Entrenamiento: Entrené el modelo durante un máximo de 100 épocas con un tamaño de lote de 32. El optimizador fue Adam y configuré callbacks como EarlyStopping (con paciencia de 10 sobre val_loss) para detener el entrenamiento si no había mejora y ReduceLROnPlateau (con paciencia de 5) para reducir la tasa de aprendizaje en caso de estancamiento.
4.2.2. Modelo de Transfer Learning: VGG19
•	Justificación: Implementé el modelo VGG19 como la primera prueba de Transfer Learning. Su arquitectura profunda y clásica permite evaluar de forma directa el beneficio de utilizar características pre-entrenadas de ImageNet en comparación con el entrenamiento desde cero.
•	Arquitectura: Utilicé el modelo VGG19 pre-entrenado en ImageNet, excluyendo su cabezal de clasificación original (include_top=False). Basado en el análisis de parámetros, donde el modelo cuenta con 20.09M de parámetros totales y 9.51M entrenables, descongelé y re-entrené las capas del bloque 5, manteniendo congeladas las capas inferiores para preservar las características de bajo nivel aprendidas de ImageNet. A esta base le añadí un cabezal de clasificación personalizado para la tarea de 8 clases.
•	Pipeline de Datos: Las imágenes de entrada se reescalaron a 224x224 píxeles para coincidir con el tamaño de entrada esperado por VGG. Apliqué la función preprocess_input específica de VGG, la cual reordena los canales de RGB a BGR y centra los valores de los píxeles según las medias del dataset ImageNet.
•	Entrenamiento: Configuré el entrenamiento con un optimizador Adam y una tasa de aprendizaje conservadora para el proceso de fine-tuning, con el objetivo de evitar el olvido catastrófico de las características pre-entrenadas.
4.2.3. Modelo de Transfer Learning: ResNet50
•	Justificación: Implementé ResNet50 para evaluar el impacto de las conexiones residuales en esta tarea. Según He, et al. [5], esta innovación arquitectónica permite entrenar redes significativamente más profundas de manera efectiva, lo que es potencialmente beneficioso para aprender las complejas morfologías del tejido histológico.
•	Arquitectura: Utilicé la base de ResNet50 pre-entrenada en ImageNet. La estrategia de fine-tuning se basó en el nombre de las capas: descongelé el modelo a partir de la capa conv5_block1_out, lo que equivale a re-entrenar los últimos 3 bloques residuales (aproximadamente 21 capas). Una decisión crítica fue mantener congeladas las 53 capas de BatchNormalization del modelo base. Esto previene cambios drásticos en la distribución de las activaciones con un tamaño de lote de 32 y mantiene estables las estadísticas aprendidas de ImageNet. El cabezal de clasificación personalizado consistió en una capa GlobalAveragePooling2D, seguida de una capa Dense de 1024 unidades, BatchNormalization, Dropout de 0.3 y la capa de salida final.
•	Pipeline de Datos: Las imágenes se reescalaron a 224x224 píxeles. Se aplicó la función preprocess_input específica para los modelos ResNet.
•	Entrenamiento: Utilicé el optimizador Adam con una tasa de aprendizaje de 5e-5. Los callbacks incluyeron EarlyStopping con paciencia de 10 y ReduceLROnPlateau con paciencia de 3 y un factor de reducción de 0.3, ambos monitoreando la pérdida de validación (val_loss).
4.2.4. Modelo de Transfer Learning: EfficientNetB0
•	Justificación: La elección de EfficientNetB0 se basó directamente en la hipótesis del proyecto. Implementé este modelo para probar una arquitectura diseñada explícitamente para lograr un balance óptimo entre precisión y eficiencia computacional a través del escalado compuesto, como proponen Tan & Le [6].
•	Arquitectura: Utilicé la base EfficientNetB0 pre-entrenada en ImageNet. La estrategia de fine-tuning consistió en descongelar, basándome en sus nombres, las capas de los bloques block6 y block7, además de las capas finales top_conv y top_bn. El cabezal de clasificación se compuso de GlobalAveragePooling2D, una capa Dense de 512 unidades, un Dropout de 0.4 y la capa de salida final, la cual configuré explícitamente con dtype='float32' para asegurar la estabilidad numérica durante el entrenamiento con precisión mixta.
•	Pipeline de Datos: Las imágenes se reescalaron a 224x224. El preprocesamiento (reescalado a [0, 1] y normalización de ImageNet) está integrado directamente en el modelo EfficientNet, lo que simplifica el pipeline.
•	Entrenamiento: Utilicé el optimizador Adam con una tasa de aprendizaje de 3.51e-05 y una regularización L2 de 0.00176, hiperparámetros que fueron obtenidos tras un proceso de optimización Bayesiana y posterior refinamiento manual.
4.2.5. Modelo de Transfer Learning: Vision Transformer (ViT-B/16)
•	Justificación: Incluí un Vision Transformer para explorar una arquitectura fundamentalmente diferente, basada en mecanismos de atención en lugar de convoluciones. El objetivo era determinar si su capacidad para modelar dependencias a larga distancia, siguiendo el trabajo de Dosovitskiy, et al. [8], ofrecía alguna ventaja en el contexto de las imágenes histológicas, donde las relaciones entre regiones distantes pueden ser diagnósticamente relevantes.
•	Arquitectura: Utilicé el modelo ViT-B/16 pre-entrenado en el dataset ImageNet-21k. La estrategia fue de transferencia pura: el backbone del Transformer se mantuvo completamente congelado para preservar sus potentes representaciones visuales, y solo entrené un cabezal de clasificación añadido. Este cabezal consistió en una capa Dense de 512 unidades con activación gelu y un Dropout de 0.1.
•	Pipeline de Datos: Las imágenes se reescalaron a 224x224. Se aplicó una normalización estilo ImageNet para llevar los valores de los píxeles al rango [-1, 1] (media=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]). A diferencia de los otros modelos, eliminé el aumento de color para evitar un desajuste de distribución con los pesos pre-entrenados del ViT, que son más sensibles a este tipo de transformaciones.
•	Entrenamiento: Utilicé el optimizador AdamW, que desacopla el decaimiento de peso de la actualización del gradiente, con una tasa de aprendizaje base de 1e-4 y un decaimiento de peso de 1e-4. Implementé un esquema de tasa de aprendizaje de decaimento por coseno con 5 épocas de "warmup", comenzando desde una tasa de 1e-5. Esta estrategia es estándar para estabilizar el entrenamiento de modelos Transformer.
La ejecución de estos cinco experimentos, cada uno con una justificación y configuración distintas, proporcionará la evidencia empírica necesaria para comparar de manera robusta las diferentes arquitecturas y validar la hipótesis del proyecto.
5. RESULTADOS
En esta sección se presentan y analizan los resultados obtenidos de la evaluación comparativa de los cinco modelos de aprendizaje profundo. El análisis abarca métricas de rendimiento de clasificación en el conjunto de prueba, un estudio de la dinámica de entrenamiento para evaluar la generalización y un análisis de la eficiencia computacional para validar las hipótesis sobre el balance costo-beneficio.
5.1. Rendimiento General de Clasificación
1.	Tabla Comparativa: A continuación, se presenta una tabla con los resultados globales de rendimiento obtenidos en el conjunto de prueba, ordenados de mayor a menor precisión.
Modelo	Test Accuracy	Macro F1-Score	Cohen's Kappa
ResNet50	0.9507	0.9510	0.9436
VGG19	0.9453	0.9457	0.9375
EfficientNetB0	0.9333	0.9338	0.9238
ViT-B/16	0.9227	0.9229	0.9116
CNN Baseline	0.9200	0.9211	0.9086
1.	Análisis de Resultados: Los resultados confirman una de las premisas centrales de la hipótesis: todos los modelos que utilizaron Transfer Learning superaron significativamente a la CNN de base entrenada desde cero. El modelo ResNet50 se posiciona como el de mejor rendimiento general, alcanzando una precisión del 95.07%, seguido de cerca por VGG19 con un 94.53%. Esto sugiere que las arquitecturas con conexiones residuales son particularmente efectivas para esta tarea de clasificación histológica.
2.	Intervalos de Confianza: Para evaluar la significancia estadística de estas diferencias, se realizó un análisis de bootstrap con 1000 iteraciones. Los intervalos de confianza del 95% para la precisión de los modelos superiores muestran un solapamiento considerable. Por ejemplo, el intervalo para ResNet50 es [0.9347, 0.9653], mientras que para VGG19 es [0.9293, 0.9613]. Este solapamiento indica que, aunque ResNet50 obtuvo una métrica puntual más alta, la diferencia de rendimiento con VGG19 podría no ser estadísticamente significativa sin realizar pruebas de hipótesis formales adicionales.
5.2. Análisis de Rendimiento por Clase
1.	Análisis de F1-Score: El análisis del F1-Score por clase revela que no todos los tipos de tejido son igualmente fáciles de clasificar. Las clases 03_COMPLEX (tejido mixto) y 02_STROMA (tejido conectivo) presentan, en general, los F1-scores más bajos en todos los modelos, lo que indica una mayor dificultad de diferenciación. El modelo ResNet50 destaca por su rendimiento consistentemente alto en casi todas las categorías, logrando puntajes F1 sobresalientes en 01_TUMOR (0.9787) y manteniendo un buen desempeño incluso en la clase más difícil, 02_STROMA (0.9101).
2.	Análisis de Matrices de Confusión: Las matrices de confusión confirman los patrones de error observados. La principal fuente de misclasificación ocurre entre las clases 03_COMPLEX, 02_STROMA y 01_TUMOR. Este hallazgo es coherente desde una perspectiva histológica, ya que estas tres clases a menudo presentan características morfológicas y texturas superpuestas, lo que las hace intrínsecamente difíciles de distinguir, incluso para un patólogo humano.
5.3. Dinámica de Entrenamiento y Generalización
1.	Curvas de Aprendizaje: Las curvas de aprendizaje comparativas muestran tendencias claras. Los modelos basados en Transfer Learning, especialmente ResNet50 y VGG19, exhiben una convergencia significativamente más rápida y estable en la precisión de validación en comparación con la CNN de base. El modelo de base, entrenado desde cero, muestra una curva más ruidosa y una convergencia más lenta, lo que subraya el beneficio de comenzar con pesos pre-entrenados.
2.	Análisis de Sobreajuste: Se analizó la brecha entre la precisión de entrenamiento y la de validación (Train-Val Gap) como un indicador de sobreajuste.
o	El ViT-B/16 muestra la menor brecha (0.0033), lo que indica una excelente capacidad de generalización, probablemente debido a que su backbone se mantuvo congelado.
o	Por el contrario, EfficientNetB0 (0.0359) y ResNet50 (0.0298) muestran las brechas más altas entre los modelos de Transfer Learning, sugiriendo un ligero sobreajuste que podría mitigarse con una regularización más fuerte.
o	La CNN Baseline presenta una anomalía con una brecha negativa (-0.0203), donde la precisión de validación es mayor que la de entrenamiento. Esto podría deberse a una regularización excesivamente fuerte o a que el aumento de datos aplicado al conjunto de entrenamiento lo hizo más "difícil" que el conjunto de validación, que no tiene aumento.
5.4. Análisis de Eficiencia Computacional
1.	Tabla Comparativa de Eficiencia: La siguiente tabla resume las métricas de eficiencia computacional para cada modelo.
Modelo	Parámetros (M)	FLOPs (G)	Tiempo de Inferencia (ms)	Throughput (img/s)
CNN Baseline	1.24	2.26	27.09	36.92
VGG19	20.09	39.04	24.62	40.62
ResNet50	25.70	7.73	181.66	5.50
EfficientNetB0	4.71	0.79	169.00	5.92
ViT-B/16	86.20	35.23	328.15	3.05
1.	Análisis de Trade-offs: El análisis de la eficiencia revela importantes disyuntivas (trade-offs):
o	Parámetros: La CNN Baseline es, por mucho, el modelo más ligero (1.24M), seguida por EfficientNetB0 (4.71M). En el extremo opuesto, el ViT-B/16 es extremadamente pesado (86.20M), lo que limita su despliegue en entornos con restricciones de memoria.
o	Costo Teórico (FLOPs): EfficientNetB0 confirma su diseño eficiente, requiriendo solo 0.79 GFLOPs por inferencia. En contraste, VGG19 es teóricamente el más costoso, con 39.04 GFLOPs.
o	Velocidad Práctica (Inferencia): Sorprendentemente, VGG19 resultó ser el modelo más rápido en la práctica (24.62 ms por imagen), mientras que ViT-B/16 fue el más lento (328.15 ms). Aquí se observa la paradoja de EfficientNetB0: a pesar de tener los FLOPs más bajos, su tiempo de inferencia es alto (169.00 ms). Esto se debe a que su arquitectura utiliza operaciones, como las convoluciones separables en profundidad, que están limitadas por el ancho de banda de la memoria (memory-bound). Estas operaciones son menos eficientes en el hardware utilizado para los experimentos (GPU NVIDIA GTX 1650), lo que demuestra que los FLOPs teóricos no siempre se traducen directamente en velocidad de inferencia práctica.
Estos resultados multifacéticos proporcionan una visión completa del rendimiento de cada arquitectura, no solo en términos de precisión, sino también de eficiencia y generalización, sentando las bases para las conclusiones finales del proyecto.
6. CONCLUSIONES Y TRABAJO FUTURO
En esta sección final, se sintetizan los hallazgos clave del proyecto, se contrasta la evidencia experimental con la hipótesis inicial para su validación, y se proponen líneas de investigación futuras que pueden expandir y profundizar el trabajo realizado.
6.1. Conclusiones y Cumplimiento de Objetivos
Se cumplieron satisfactoriamente todos los objetivos específicos planteados al inicio del proyecto:
1.	Sobre la Implementación (Objetivo 1): Se implementaron, entrenaron y evaluaron con éxito cinco arquitecturas de aprendizaje profundo distintas. Se incluyó un modelo de base entrenado desde cero (CNN Baseline) y cuatro modelos aplicando Transfer Learning (VGG19, ResNet50, EfficientNetB0, ViT-B/16), cubriendo así un espectro representativo y diverso de los enfoques actuales en visión computacional.
2.	Sobre la Evaluación de Rendimiento (Objetivo 2): La evaluación de rendimiento demostró de manera concluyente la superioridad del enfoque de Transfer Learning sobre el entrenamiento desde cero para este conjunto de datos. ResNet50 se identificó como el modelo con la mayor precisión de clasificación (95.07%). Adicionalmente, el análisis de las curvas de aprendizaje reveló la excelente capacidad de generalización del ViT-B/16, que, gracias a su estrategia de entrenamiento con backbone congelado, presentó la menor brecha de sobreajuste.
3.	Sobre la Medición de Eficiencia (Objetivo 3): La medición de la eficiencia computacional expuso disyuntivas críticas entre el costo teórico y el rendimiento práctico. EfficientNetB0 se confirmó como el modelo más eficiente en teoría (0.79 GFLOPs y 4.71M de parámetros). Sin embargo, VGG19 fue el más rápido en la práctica en el hardware disponible (24.62 ms/imagen). Este hallazgo subraya la importancia de realizar evaluaciones empíricas de la latencia, ya que métricas teóricas como los FLOPs pueden no reflejar el rendimiento real debido a factores como la arquitectura del hardware y las operaciones limitadas por memoria.
4.	Sobre la Relación Costo-Beneficio (Objetivo 4): Se determinó que no existe una única arquitectura "mejor" de manera absoluta; la elección óptima depende de las prioridades del caso de uso. Para maximizar la precisión, ResNet50 es la mejor opción. Para el mejor balance entre precisión y eficiencia de parámetros, EfficientNetB0 es la alternativa más prometedora. Si la velocidad de inferencia es la máxima prioridad, VGG19 es la opción preferible. Finalmente, para entornos con restricciones extremas de memoria, la CNN Baseline ofrece un rendimiento aceptable con un tamaño mínimo.
6.2. Validación de la Hipótesis
La hipótesis inicial fue evaluada punto por punto con base en la evidencia experimental:
•	La afirmación de que "la aplicación de técnicas de Transfer Learning (...) permitirá alcanzar niveles de precisión superiores" se confirma plenamente. Los cuatro modelos de Transfer Learning superaron a la CNN entrenada desde cero en todas las métricas de rendimiento.
•	La segunda parte, que postula que "las arquitecturas CNN optimizadas (EfficientNet) ofrecerán el mejor balance entre rendimiento de clasificación y eficiencia computacional", se confirma parcialmente. EfficientNetB0 (93.33% de precisión, 0.79 GFLOPs) efectivamente demostró un excelente balance, pero fue superado en precisión absoluta por ResNet50 (95.07%, 7.73 GFLOPs) y en velocidad de inferencia por VGG19. Por lo tanto, ofrece un balance "prometedor", pero no el "mejor" en todas las dimensiones simultáneamente.
•	Finalmente, la predicción de que estas superarían a "arquitecturas pesadas (VGG) o Vision Transformers, que requerirán costo computacional desproporcionado", se confirma, especialmente en el caso del ViT-B/16. Con 86.2M de parámetros y 328 ms de tiempo de inferencia, su costo computacional fue muy alto para una ganancia de precisión que no fue competitiva frente a las mejores CNNs.
6.3. Trabajo Futuro
Basado en los hallazgos y limitaciones de este proyecto, se proponen las siguientes líneas de investigación futuras:
•	Exploración de Arquitecturas Modernas: Evaluar arquitecturas más recientes como EfficientNetV2, que promete mejoras en velocidad de entrenamiento y eficiencia, o arquitecturas híbridas CNN-Transformer (ej. ConViT), que buscan combinar el sesgo inductivo local de las convoluciones con la capacidad de modelado global de los mecanismos de atención.
•	Pre-entrenamiento Específico del Dominio: Investigar el uso de modelos pre-entrenados en grandes datasets de histopatología (ej. PathMNIST) en lugar de ImageNet. Esto podría reducir la brecha de dominio entre las imágenes naturales y las médicas, potencialmente mejorando el rendimiento del Transfer Learning.
•	Entrenamiento con Mayor Resolución: Experimentar con tamaños de imagen de entrada más grandes (ej. 384x384). Esto podría permitir a los modelos, especialmente a EfficientNet y ViT que están diseñados para escalar en resolución, capturar detalles morfológicos más finos que mejoren la clasificación de clases complejas.
•	Análisis de Interpretabilidad: Aplicar técnicas de explicabilidad como Grad-CAM++ en los modelos CNN y analizar los mapas de atención nativos del ViT. El objetivo es visualizar qué regiones y características del tejido son más relevantes para las predicciones del modelo, lo que podría aumentar la confianza del patólogo y facilitar la validación clínica de los resultados.
•	Métodos de Ensamblaje: Combinar las predicciones de los modelos con mejor desempeño individual (ej. ResNet50 y EfficientNetB0) mediante técnicas de ensamblaje (stacking o promediado ponderado). El objetivo sería explorar si la combinación de modelos con diferentes fortalezas puede superar el rendimiento de cualquier modelo individual.
7. BIBLIOGRAFÍA
1.	Kather, J. N., Weis, C. A., Bianconi, F., Melchers, S. M., Schad, L. R., Gaiser, T., Marx, A., & Zollner, F. G. (2016). Multi-class texture analysis in colorectal cancer histology. Scientific Reports, 6, 27988.
2.	Tellez, D., Balkenhol, M., Otte-Höller, I., van de Loo, R., Vogels, R., Bult, P., ... & van der Laak, J. (2019). Quantifying the effects of data augmentation and stain color normalization in convolutional neural networks for computational pathology. Medical Image Analysis, 58, 101544.
3.	Yamashita, R., Nishio, M., Do, R. K. G., & Togashi, K. (2018). Convolutional neural networks: an overview and application in radiology. Insights into Imaging, 9(4), 611-629.
4.	Simonyan, K., & Zisserman, A. (2014). Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556.
5.	He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. En Actas de la Conferencia de la IEEE sobre Visión por Computadora y Reconocimiento de Patrones (CVPR) (pp. 770-778).
6.	Tan, M., & Le, Q. V. (2019). EfficientNet: Rethinking model scaling for convolutional neural networks. En Conferencia Internacional sobre Aprendizaje Automático (ICML) (pp. 6105-6114). PMLR.
7.	Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, Ł., & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.
8.	Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., ... & Houlsby, N. (2021). An image is worth 16x16 words: Transformers for image recognition at scale. En Conferencia Internacional sobre Representaciones del Aprendizaje (ICLR).
9.	Loshchilov, I., & Hutter, F. (2017). SGDR: Stochastic gradient descent with warm restarts. En Conferencia Internacional sobre Representaciones del Aprendizaje (ICLR).
10.	Goyal, P., Dollár, P., Girshick, R., Noordhuis, P., Wesolowski, L., Kyrola, A., ... & He, K. (2017). Accurate, large minibatch sgd: Training imagenet in 1 hour. arXiv preprint arXiv:1706.02677.
